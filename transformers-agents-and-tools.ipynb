{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/mastersniffer/transformers-agents-and-tools?scriptVersionId=227440346\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"!pip install --upgrade --quiet transformers datasets evaluate soundfile librosa  \n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-03-13T16:37:51.970594Z","iopub.execute_input":"2025-03-13T16:37:51.970861Z","iopub.status.idle":"2025-03-13T16:38:34.528242Z","shell.execute_reply.started":"2025-03-13T16:37:51.970836Z","shell.execute_reply":"2025-03-13T16:38:34.526653Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.0/44.0 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.0/10.0 MB\u001b[0m \u001b[31m45.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m485.4/485.4 kB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m26.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m260.7/260.7 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"# Agents and tools\n\nWhat is an agent?\nLarge Language Models (LLMs) trained to perform causal language modeling can tackle a wide range of tasks, but they often struggle with basic tasks like logic, calculation, and search. When prompted in domains in which they do not perform well, they often fail to generate the answer we expect them to.\n\nOne approach to overcome this weakness is to create an agent.\n\nAn agent is a system that uses an LLM as its engine, and it has access to functions called tools.\n\nThese tools are functions for performing a task, and they contain all necessary description for the agent to properly use them.\n\nThe agent can be programmed to:\n\ndevise a series of actions/tools and run them all at once, like the CodeAgent\nplan and execute actions/tools one by one and wait for the outcome of each action before launching the next one, like the ReactJsonAgent\nTypes of agents\nCode agent\nThis agent has a planning step, then generates python code to execute all its actions at once. It natively handles different input and output types for its tools, thus it is the recommended choice for multimodal tasks.\n","metadata":{}},{"cell_type":"markdown","source":"# How can I build an agent?\nTo initialize an agent, you need these arguments:\n\nan LLM to power your agent - the agent is not exactly the LLM, it’s more like the agent is a program that uses an LLM as its engine.\na system prompt: what the LLM engine will be prompted with to generate its output\na toolbox from which the agent pick tools to execute\na parser to extract from the LLM output which tools are to call and with which arguments\nUpon initialization of the agent system, the tool attributes are used to generate a tool description, then baked into the agent’s system_prompt to let it know which tools it can use and why.","metadata":{}},{"cell_type":"code","source":"!pip install transformers[agents] -qU","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-13T16:40:59.51375Z","iopub.execute_input":"2025-03-13T16:40:59.514142Z","iopub.status.idle":"2025-03-13T16:41:04.281361Z","shell.execute_reply.started":"2025-03-13T16:40:59.514108Z","shell.execute_reply":"2025-03-13T16:41:04.279879Z"}},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":"Build your LLM engine by defining a llm_engine method which accepts a list of messages and returns text. This callable also needs to accept a stop argument that indicates when to stop generating.","metadata":{}},{"cell_type":"code","source":"from huggingface_hub import login, InferenceClient\n\nlogin(\"...\")\n\nclient = InferenceClient(model=\"meta-llama/Meta-Llama-3-70B-Instruct\")\n\ndef llm_engine(messages, stop_sequences=[\"Task\"]) -> str:\n    response = client.chat_completion(messages, stop=stop_sequences, max_tokens=1000)\n    answer = response.choices[0].message.content\n    return answer","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-13T16:59:14.648482Z","iopub.execute_input":"2025-03-13T16:59:14.648913Z","iopub.status.idle":"2025-03-13T16:59:15.871878Z","shell.execute_reply.started":"2025-03-13T16:59:14.648879Z","shell.execute_reply":"2025-03-13T16:59:15.870226Z"}},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":"You could use any llm_engine method as long as:\n\nit follows the messages format (List[Dict[str, str]]) for its input messages, and it returns a str.\nit stops generating outputs at the sequences passed in the argument stop_sequences\nAdditionally, llm_engine can also take a grammar argument. In the case where you specify a grammar upon agent initialization, this argument will be passed to the calls to llm_engine, with the grammar that you defined upon initialization, to allow constrained generation in order to force properly-formatted agent outputs.\n\nYou will also need a tools argument which accepts a list of Tools - it can be an empty list. You can also add the default toolbox on top of your tools list by defining the optional argument add_base_tools=True.\n\nNow you can create an agent, like CodeAgent, and run it. You can also create a TransformersEngine with a pre-initialized pipeline to run inference on your local machine using transformers. For convenience, since agentic behaviours generally require stronger models such as Llama-3.1-70B-Instruct that are harder to run locally for now, we also provide the HfApiEngine class that initializes a huggingface_hub.InferenceClient under the hood.","metadata":{}},{"cell_type":"code","source":"from transformers import CodeAgent, HfApiEngine\n\nllm_engine = HfApiEngine(model=\"meta-llama/Meta-Llama-3-70B-Instruct\")\nagent = CodeAgent(tools=[], llm_engine=llm_engine, add_base_tools=True)\n\nagent.run(\n    \"Could you translate this sentence from French, say it out loud and return the audio.\",\n    sentence=\"Où est la boulangerie la plus proche?\",\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-13T17:03:16.734291Z","iopub.execute_input":"2025-03-13T17:03:16.734662Z","iopub.status.idle":"2025-03-13T17:03:54.982821Z","shell.execute_reply.started":"2025-03-13T17:03:16.734633Z","shell.execute_reply":"2025-03-13T17:03:54.980982Z"}},"outputs":[{"name":"stderr","text":"Failed to load tokenizer for model meta-llama/Meta-Llama-3-70B-Instruct: You are trying to access a gated repo.\nMake sure to have access to it at https://huggingface.co/meta-llama/Meta-Llama-3-70B-Instruct.\n403 Client Error. (Request ID: Root=1-67d30ff4-7b1d302b6194877049cd5fc8;340d352a-18ea-4b7f-be8c-20b2526923a3)\n\nCannot access gated repo for url https://huggingface.co/meta-llama/Meta-Llama-3-70B-Instruct/resolve/main/config.json.\nAccess to model meta-llama/Meta-Llama-3-70B-Instruct is restricted and you are not in the authorized list. Visit https://huggingface.co/meta-llama/Meta-Llama-3-70B-Instruct to ask for access.. Loading default tokenizer instead.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/3.76k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d88f5dcabaff485db4bb31d173ead370"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/801k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e128680732c34bc68210079ed27c57bc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0ecaa5f2e22c427f9db425e0927c1040"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/2.10M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"97834c94b66544f5b16e415b844c66f7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/655 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ed0a6dbe89f6465facb6e2d5f67bdf0c"}},"metadata":{}},{"name":"stderr","text":"\u001b[32;20;1m======== New task ========\u001b[0m\n\u001b[37;1mCould you translate this sentence from French, say it out loud and return the audio.\nYou have been provided with these initial arguments: {'sentence': 'Où est la boulangerie la plus proche?'}.\u001b[0m\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_http.py\u001b[0m in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    408\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 409\u001b[0;31m         \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_for_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    410\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mHTTPError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/requests/models.py\u001b[0m in \u001b[0;36mraise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1023\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhttp_error_msg\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1024\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mHTTPError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhttp_error_msg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mHTTPError\u001b[0m: 400 Client Error: Bad Request for url: https://router.huggingface.co/hf-inference/models/meta-llama/Meta-Llama-3-70B-Instruct/v1/chat/completions","\nThe above exception was the direct cause of the following exception:\n","\u001b[0;31mBadRequestError\u001b[0m                           Traceback (most recent call last)","\u001b[0;32m<ipython-input-4-788fc4cd22f1>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0magent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCodeAgent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtools\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mllm_engine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mllm_engine\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_base_tools\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m agent.run(\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0;34m\"Could you translate this sentence from French, say it out loud and return the audio.\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0msentence\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Où est la boulangerie la plus proche?\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/agents/agents.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, task, return_generated_code, **kwargs)\u001b[0m\n\u001b[1;32m    674\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    675\u001b[0m         \u001b[0madditional_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"grammar\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrammar\u001b[0m\u001b[0;34m}\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrammar\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 676\u001b[0;31m         \u001b[0mllm_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mllm_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop_sequences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"<end_action>\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0madditional_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    678\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mreturn_generated_code\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/agents/llm_engine.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, messages, stop_sequences, grammar)\u001b[0m\n\u001b[1;32m    132\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstop_sequences\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m             \u001b[0mstop_sequences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop_sequences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrammar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast_input_token_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_chat_template\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast_output_token_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/agents/llm_engine.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, messages, stop_sequences, grammar)\u001b[0m\n\u001b[1;32m    188\u001b[0m             )\n\u001b[1;32m    189\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 190\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchat_completion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop_sequences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_tokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    191\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/inference/_client.py\u001b[0m in \u001b[0;36mchat_completion\u001b[0;34m(self, messages, model, stream, frequency_penalty, logit_bias, logprobs, max_tokens, n, presence_penalty, response_format, seed, stop, stream_options, temperature, tool_choice, tool_prompt, tools, top_logprobs, top_p, extra_body)\u001b[0m\n\u001b[1;32m    954\u001b[0m             \u001b[0mapi_key\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    955\u001b[0m         )\n\u001b[0;32m--> 956\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inner_post\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_parameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    957\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    958\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/inference/_client.py\u001b[0m in \u001b[0;36m_inner_post\u001b[0;34m(self, request_parameters, stream)\u001b[0m\n\u001b[1;32m    319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 321\u001b[0;31m                 \u001b[0mhf_raise_for_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    322\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miter_lines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mstream\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mHTTPError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_http.py\u001b[0m in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    462\u001b[0m                 \u001b[0;34mf\"\\n\\nBad request for {endpoint_name} endpoint:\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mendpoint_name\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"\\n\\nBad request:\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    463\u001b[0m             )\n\u001b[0;32m--> 464\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0m_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBadRequestError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    465\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    466\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m403\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mBadRequestError\u001b[0m: (Request ID: Root=1-67d30ffa-4312c5675486b64652bc983d;e0b0c80f-43c5-44e0-86c8-736be012d319)\n\nBad request:\nModel requires a Pro subscription; check out hf.co/pricing to learn more. Make sure to include your HF token in your query."],"ename":"BadRequestError","evalue":"(Request ID: Root=1-67d30ffa-4312c5675486b64652bc983d;e0b0c80f-43c5-44e0-86c8-736be012d319)\n\nBad request:\nModel requires a Pro subscription; check out hf.co/pricing to learn more. Make sure to include your HF token in your query.","output_type":"error"}],"execution_count":4},{"cell_type":"code","source":"from transformers import CodeAgent\n\nagent = CodeAgent(tools=[], add_base_tools=True)\n\nagent.run(\n    \"Could you translate this sentence from French, say it out loud and give me the audio.\",\n    sentence=\"Où est la boulangerie la plus proche?\",\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-13T17:05:08.529821Z","iopub.execute_input":"2025-03-13T17:05:08.53025Z","iopub.status.idle":"2025-03-13T17:05:09.559192Z","shell.execute_reply.started":"2025-03-13T17:05:08.530221Z","shell.execute_reply":"2025-03-13T17:05:09.557666Z"}},"outputs":[{"name":"stderr","text":"Failed to load tokenizer for model meta-llama/Meta-Llama-3.1-8B-Instruct: You are trying to access a gated repo.\nMake sure to have access to it at https://huggingface.co/meta-llama/Meta-Llama-3.1-8B-Instruct.\n403 Client Error. (Request ID: Root=1-67d31044-2d6e35a26f1893134f93e9bc;9d83f376-2a27-448f-9ac5-9b5f8a95caca)\n\nCannot access gated repo for url https://huggingface.co/meta-llama/Meta-Llama-3.1-8B-Instruct/resolve/main/config.json.\nAccess to model meta-llama/Llama-3.1-8B-Instruct is restricted and you are not in the authorized list. Visit https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct to ask for access.. Loading default tokenizer instead.\n\u001b[32;20;1m======== New task ========\u001b[0m\n\u001b[37;1mCould you translate this sentence from French, say it out loud and give me the audio.\nYou have been provided with these initial arguments: {'sentence': 'Où est la boulangerie la plus proche?'}.\u001b[0m\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_http.py\u001b[0m in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    408\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 409\u001b[0;31m         \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_for_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    410\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mHTTPError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/requests/models.py\u001b[0m in \u001b[0;36mraise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1023\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhttp_error_msg\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1024\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mHTTPError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhttp_error_msg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mHTTPError\u001b[0m: 400 Client Error: Bad Request for url: https://router.huggingface.co/hf-inference/models/meta-llama/Meta-Llama-3.1-8B-Instruct/v1/chat/completions","\nThe above exception was the direct cause of the following exception:\n","\u001b[0;31mBadRequestError\u001b[0m                           Traceback (most recent call last)","\u001b[0;32m<ipython-input-5-4f50fb41c12c>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0magent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCodeAgent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtools\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_base_tools\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m agent.run(\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0;34m\"Could you translate this sentence from French, say it out loud and give me the audio.\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0msentence\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Où est la boulangerie la plus proche?\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/agents/agents.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, task, return_generated_code, **kwargs)\u001b[0m\n\u001b[1;32m    674\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    675\u001b[0m         \u001b[0madditional_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"grammar\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrammar\u001b[0m\u001b[0;34m}\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrammar\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 676\u001b[0;31m         \u001b[0mllm_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mllm_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop_sequences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"<end_action>\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0madditional_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    678\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mreturn_generated_code\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/agents/llm_engine.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, messages, stop_sequences, grammar)\u001b[0m\n\u001b[1;32m    132\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstop_sequences\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m             \u001b[0mstop_sequences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop_sequences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrammar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast_input_token_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_chat_template\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast_output_token_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/agents/llm_engine.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, messages, stop_sequences, grammar)\u001b[0m\n\u001b[1;32m    188\u001b[0m             )\n\u001b[1;32m    189\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 190\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchat_completion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop_sequences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_tokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    191\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/inference/_client.py\u001b[0m in \u001b[0;36mchat_completion\u001b[0;34m(self, messages, model, stream, frequency_penalty, logit_bias, logprobs, max_tokens, n, presence_penalty, response_format, seed, stop, stream_options, temperature, tool_choice, tool_prompt, tools, top_logprobs, top_p, extra_body)\u001b[0m\n\u001b[1;32m    954\u001b[0m             \u001b[0mapi_key\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    955\u001b[0m         )\n\u001b[0;32m--> 956\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inner_post\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_parameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    957\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    958\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/inference/_client.py\u001b[0m in \u001b[0;36m_inner_post\u001b[0;34m(self, request_parameters, stream)\u001b[0m\n\u001b[1;32m    319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 321\u001b[0;31m                 \u001b[0mhf_raise_for_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    322\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miter_lines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mstream\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mHTTPError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_http.py\u001b[0m in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    462\u001b[0m                 \u001b[0;34mf\"\\n\\nBad request for {endpoint_name} endpoint:\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mendpoint_name\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"\\n\\nBad request:\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    463\u001b[0m             )\n\u001b[0;32m--> 464\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0m_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBadRequestError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    465\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    466\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m403\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mBadRequestError\u001b[0m: (Request ID: Root=1-67d31045-5340732628ca2be2505d696c;f44227df-3475-4110-831f-b7f769fd718a)\n\nBad request:\nModel requires a Pro subscription; check out hf.co/pricing to learn more. Make sure to include your HF token in your query."],"ename":"BadRequestError","evalue":"(Request ID: Root=1-67d31045-5340732628ca2be2505d696c;f44227df-3475-4110-831f-b7f769fd718a)\n\nBad request:\nModel requires a Pro subscription; check out hf.co/pricing to learn more. Make sure to include your HF token in your query.","output_type":"error"}],"execution_count":5},{"cell_type":"markdown","source":"Note that we used an additional sentence argument: you can pass text as additional arguments to the model.\n\nYou can also use this to indicate the path to local or remote files for the model to use:","metadata":{}},{"cell_type":"code","source":"from transformers import ReactCodeAgent\n\nagent = ReactCodeAgent(tools=[], llm_engine=llm_engine, add_base_tools=True)\n\nagent.run(\"Why does Mike not know many people in New York?\", audio=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/recording.mp3\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-13T17:05:21.121352Z","iopub.execute_input":"2025-03-13T17:05:21.121755Z","iopub.status.idle":"2025-03-13T17:05:22.894273Z","shell.execute_reply.started":"2025-03-13T17:05:21.121722Z","shell.execute_reply":"2025-03-13T17:05:22.893045Z"}},"outputs":[{"name":"stderr","text":"\u001b[32;20;1m======== New task ========\u001b[0m\n\u001b[37;1mWhy does Mike not know many people in New York?\nYou have been provided with these initial arguments: {'audio': 'https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/recording.mp3'}.\u001b[0m\n\u001b[31;20mError in generating llm output: (Request ID: Root=1-67d31051-6fddebd20807e12f7ea2d600;66da31f2-86c4-438c-9858-ffd5027452d8)\n\nBad request:\nModel requires a Pro subscription; check out hf.co/pricing to learn more. Make sure to include your HF token in your query..\u001b[0m\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_http.py\", line 409, in hf_raise_for_status\n    response.raise_for_status()\n  File \"/usr/local/lib/python3.10/dist-packages/requests/models.py\", line 1024, in raise_for_status\n    raise HTTPError(http_error_msg, response=self)\nrequests.exceptions.HTTPError: 400 Client Error: Bad Request for url: https://router.huggingface.co/hf-inference/models/meta-llama/Meta-Llama-3-70B-Instruct/v1/chat/completions\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/agents/agents.py\", line 1160, in step\n    llm_output = self.llm_engine(\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/agents/llm_engine.py\", line 134, in __call__\n    response = self.generate(messages, stop_sequences, grammar)\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/agents/llm_engine.py\", line 190, in generate\n    response = self.client.chat_completion(messages, stop=stop_sequences, max_tokens=self.max_tokens)\n  File \"/usr/local/lib/python3.10/dist-packages/huggingface_hub/inference/_client.py\", line 956, in chat_completion\n    data = self._inner_post(request_parameters, stream=stream)\n  File \"/usr/local/lib/python3.10/dist-packages/huggingface_hub/inference/_client.py\", line 321, in _inner_post\n    hf_raise_for_status(response)\n  File \"/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_http.py\", line 464, in hf_raise_for_status\n    raise _format(BadRequestError, message, response) from e\nhuggingface_hub.errors.BadRequestError: (Request ID: Root=1-67d31051-6fddebd20807e12f7ea2d600;66da31f2-86c4-438c-9858-ffd5027452d8)\n\nBad request:\nModel requires a Pro subscription; check out hf.co/pricing to learn more. Make sure to include your HF token in your query.\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/agents/agents.py\", line 856, in direct_run\n    self.step(step_log_entry)\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/agents/agents.py\", line 1164, in step\n    raise AgentGenerationError(f\"Error in generating llm output: {e}.\")\ntransformers.agents.agents.AgentGenerationError: Error in generating llm output: (Request ID: Root=1-67d31051-6fddebd20807e12f7ea2d600;66da31f2-86c4-438c-9858-ffd5027452d8)\n\nBad request:\nModel requires a Pro subscription; check out hf.co/pricing to learn more. Make sure to include your HF token in your query..\n\u001b[31;20mError in generating llm output: (Request ID: Root=1-67d31051-21abe7f8651b341a41d976b9;a562c290-7bdf-43e9-925a-31ce41b78624)\n\nBad request:\nModel requires a Pro subscription; check out hf.co/pricing to learn more. Make sure to include your HF token in your query..\u001b[0m\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_http.py\", line 409, in hf_raise_for_status\n    response.raise_for_status()\n  File \"/usr/local/lib/python3.10/dist-packages/requests/models.py\", line 1024, in raise_for_status\n    raise HTTPError(http_error_msg, response=self)\nrequests.exceptions.HTTPError: 400 Client Error: Bad Request for url: https://router.huggingface.co/hf-inference/models/meta-llama/Meta-Llama-3-70B-Instruct/v1/chat/completions\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/agents/agents.py\", line 1160, in step\n    llm_output = self.llm_engine(\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/agents/llm_engine.py\", line 134, in __call__\n    response = self.generate(messages, stop_sequences, grammar)\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/agents/llm_engine.py\", line 190, in generate\n    response = self.client.chat_completion(messages, stop=stop_sequences, max_tokens=self.max_tokens)\n  File \"/usr/local/lib/python3.10/dist-packages/huggingface_hub/inference/_client.py\", line 956, in chat_completion\n    data = self._inner_post(request_parameters, stream=stream)\n  File \"/usr/local/lib/python3.10/dist-packages/huggingface_hub/inference/_client.py\", line 321, in _inner_post\n    hf_raise_for_status(response)\n  File \"/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_http.py\", line 464, in hf_raise_for_status\n    raise _format(BadRequestError, message, response) from e\nhuggingface_hub.errors.BadRequestError: (Request ID: Root=1-67d31051-21abe7f8651b341a41d976b9;a562c290-7bdf-43e9-925a-31ce41b78624)\n\nBad request:\nModel requires a Pro subscription; check out hf.co/pricing to learn more. Make sure to include your HF token in your query.\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/agents/agents.py\", line 856, in direct_run\n    self.step(step_log_entry)\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/agents/agents.py\", line 1164, in step\n    raise AgentGenerationError(f\"Error in generating llm output: {e}.\")\ntransformers.agents.agents.AgentGenerationError: Error in generating llm output: (Request ID: Root=1-67d31051-21abe7f8651b341a41d976b9;a562c290-7bdf-43e9-925a-31ce41b78624)\n\nBad request:\nModel requires a Pro subscription; check out hf.co/pricing to learn more. Make sure to include your HF token in your query..\n\u001b[31;20mError in generating llm output: (Request ID: Root=1-67d31051-37b268976f4a22d47c4eb6ca;26e68b85-673f-40c3-b615-24c209de819d)\n\nBad request:\nModel requires a Pro subscription; check out hf.co/pricing to learn more. Make sure to include your HF token in your query..\u001b[0m\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_http.py\", line 409, in hf_raise_for_status\n    response.raise_for_status()\n  File \"/usr/local/lib/python3.10/dist-packages/requests/models.py\", line 1024, in raise_for_status\n    raise HTTPError(http_error_msg, response=self)\nrequests.exceptions.HTTPError: 400 Client Error: Bad Request for url: https://router.huggingface.co/hf-inference/models/meta-llama/Meta-Llama-3-70B-Instruct/v1/chat/completions\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/agents/agents.py\", line 1160, in step\n    llm_output = self.llm_engine(\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/agents/llm_engine.py\", line 134, in __call__\n    response = self.generate(messages, stop_sequences, grammar)\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/agents/llm_engine.py\", line 190, in generate\n    response = self.client.chat_completion(messages, stop=stop_sequences, max_tokens=self.max_tokens)\n  File \"/usr/local/lib/python3.10/dist-packages/huggingface_hub/inference/_client.py\", line 956, in chat_completion\n    data = self._inner_post(request_parameters, stream=stream)\n  File \"/usr/local/lib/python3.10/dist-packages/huggingface_hub/inference/_client.py\", line 321, in _inner_post\n    hf_raise_for_status(response)\n  File \"/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_http.py\", line 464, in hf_raise_for_status\n    raise _format(BadRequestError, message, response) from e\nhuggingface_hub.errors.BadRequestError: (Request ID: Root=1-67d31051-37b268976f4a22d47c4eb6ca;26e68b85-673f-40c3-b615-24c209de819d)\n\nBad request:\nModel requires a Pro subscription; check out hf.co/pricing to learn more. Make sure to include your HF token in your query.\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/agents/agents.py\", line 856, in direct_run\n    self.step(step_log_entry)\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/agents/agents.py\", line 1164, in step\n    raise AgentGenerationError(f\"Error in generating llm output: {e}.\")\ntransformers.agents.agents.AgentGenerationError: Error in generating llm output: (Request ID: Root=1-67d31051-37b268976f4a22d47c4eb6ca;26e68b85-673f-40c3-b615-24c209de819d)\n\nBad request:\nModel requires a Pro subscription; check out hf.co/pricing to learn more. Make sure to include your HF token in your query..\n\u001b[31;20mError in generating llm output: (Request ID: Root=1-67d31051-60829f6147d41cc9352489ee;5ac9ba7e-eda7-48a0-a647-30e1f6825256)\n\nBad request:\nModel requires a Pro subscription; check out hf.co/pricing to learn more. Make sure to include your HF token in your query..\u001b[0m\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_http.py\", line 409, in hf_raise_for_status\n    response.raise_for_status()\n  File \"/usr/local/lib/python3.10/dist-packages/requests/models.py\", line 1024, in raise_for_status\n    raise HTTPError(http_error_msg, response=self)\nrequests.exceptions.HTTPError: 400 Client Error: Bad Request for url: https://router.huggingface.co/hf-inference/models/meta-llama/Meta-Llama-3-70B-Instruct/v1/chat/completions\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/agents/agents.py\", line 1160, in step\n    llm_output = self.llm_engine(\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/agents/llm_engine.py\", line 134, in __call__\n    response = self.generate(messages, stop_sequences, grammar)\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/agents/llm_engine.py\", line 190, in generate\n    response = self.client.chat_completion(messages, stop=stop_sequences, max_tokens=self.max_tokens)\n  File \"/usr/local/lib/python3.10/dist-packages/huggingface_hub/inference/_client.py\", line 956, in chat_completion\n    data = self._inner_post(request_parameters, stream=stream)\n  File \"/usr/local/lib/python3.10/dist-packages/huggingface_hub/inference/_client.py\", line 321, in _inner_post\n    hf_raise_for_status(response)\n  File \"/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_http.py\", line 464, in hf_raise_for_status\n    raise _format(BadRequestError, message, response) from e\nhuggingface_hub.errors.BadRequestError: (Request ID: Root=1-67d31051-60829f6147d41cc9352489ee;5ac9ba7e-eda7-48a0-a647-30e1f6825256)\n\nBad request:\nModel requires a Pro subscription; check out hf.co/pricing to learn more. Make sure to include your HF token in your query.\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/agents/agents.py\", line 856, in direct_run\n    self.step(step_log_entry)\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/agents/agents.py\", line 1164, in step\n    raise AgentGenerationError(f\"Error in generating llm output: {e}.\")\ntransformers.agents.agents.AgentGenerationError: Error in generating llm output: (Request ID: Root=1-67d31051-60829f6147d41cc9352489ee;5ac9ba7e-eda7-48a0-a647-30e1f6825256)\n\nBad request:\nModel requires a Pro subscription; check out hf.co/pricing to learn more. Make sure to include your HF token in your query..\n\u001b[31;20mError in generating llm output: (Request ID: Root=1-67d31052-2cf91486732850964f3365d2;f15fb483-0fe5-40f5-b8d0-34f98555f31e)\n\nBad request:\nModel requires a Pro subscription; check out hf.co/pricing to learn more. Make sure to include your HF token in your query..\u001b[0m\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_http.py\", line 409, in hf_raise_for_status\n    response.raise_for_status()\n  File \"/usr/local/lib/python3.10/dist-packages/requests/models.py\", line 1024, in raise_for_status\n    raise HTTPError(http_error_msg, response=self)\nrequests.exceptions.HTTPError: 400 Client Error: Bad Request for url: https://router.huggingface.co/hf-inference/models/meta-llama/Meta-Llama-3-70B-Instruct/v1/chat/completions\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/agents/agents.py\", line 1160, in step\n    llm_output = self.llm_engine(\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/agents/llm_engine.py\", line 134, in __call__\n    response = self.generate(messages, stop_sequences, grammar)\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/agents/llm_engine.py\", line 190, in generate\n    response = self.client.chat_completion(messages, stop=stop_sequences, max_tokens=self.max_tokens)\n  File \"/usr/local/lib/python3.10/dist-packages/huggingface_hub/inference/_client.py\", line 956, in chat_completion\n    data = self._inner_post(request_parameters, stream=stream)\n  File \"/usr/local/lib/python3.10/dist-packages/huggingface_hub/inference/_client.py\", line 321, in _inner_post\n    hf_raise_for_status(response)\n  File \"/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_http.py\", line 464, in hf_raise_for_status\n    raise _format(BadRequestError, message, response) from e\nhuggingface_hub.errors.BadRequestError: (Request ID: Root=1-67d31052-2cf91486732850964f3365d2;f15fb483-0fe5-40f5-b8d0-34f98555f31e)\n\nBad request:\nModel requires a Pro subscription; check out hf.co/pricing to learn more. Make sure to include your HF token in your query.\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/agents/agents.py\", line 856, in direct_run\n    self.step(step_log_entry)\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/agents/agents.py\", line 1164, in step\n    raise AgentGenerationError(f\"Error in generating llm output: {e}.\")\ntransformers.agents.agents.AgentGenerationError: Error in generating llm output: (Request ID: Root=1-67d31052-2cf91486732850964f3365d2;f15fb483-0fe5-40f5-b8d0-34f98555f31e)\n\nBad request:\nModel requires a Pro subscription; check out hf.co/pricing to learn more. Make sure to include your HF token in your query..\n\u001b[31;20mError in generating llm output: (Request ID: Root=1-67d31052-7fe470d11a8dc75b1f9c9759;9e44cc62-a92f-4c24-b273-469588ea205c)\n\nBad request:\nModel requires a Pro subscription; check out hf.co/pricing to learn more. Make sure to include your HF token in your query..\u001b[0m\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_http.py\", line 409, in hf_raise_for_status\n    response.raise_for_status()\n  File \"/usr/local/lib/python3.10/dist-packages/requests/models.py\", line 1024, in raise_for_status\n    raise HTTPError(http_error_msg, response=self)\nrequests.exceptions.HTTPError: 400 Client Error: Bad Request for url: https://router.huggingface.co/hf-inference/models/meta-llama/Meta-Llama-3-70B-Instruct/v1/chat/completions\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/agents/agents.py\", line 1160, in step\n    llm_output = self.llm_engine(\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/agents/llm_engine.py\", line 134, in __call__\n    response = self.generate(messages, stop_sequences, grammar)\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/agents/llm_engine.py\", line 190, in generate\n    response = self.client.chat_completion(messages, stop=stop_sequences, max_tokens=self.max_tokens)\n  File \"/usr/local/lib/python3.10/dist-packages/huggingface_hub/inference/_client.py\", line 956, in chat_completion\n    data = self._inner_post(request_parameters, stream=stream)\n  File \"/usr/local/lib/python3.10/dist-packages/huggingface_hub/inference/_client.py\", line 321, in _inner_post\n    hf_raise_for_status(response)\n  File \"/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_http.py\", line 464, in hf_raise_for_status\n    raise _format(BadRequestError, message, response) from e\nhuggingface_hub.errors.BadRequestError: (Request ID: Root=1-67d31052-7fe470d11a8dc75b1f9c9759;9e44cc62-a92f-4c24-b273-469588ea205c)\n\nBad request:\nModel requires a Pro subscription; check out hf.co/pricing to learn more. Make sure to include your HF token in your query.\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/agents/agents.py\", line 856, in direct_run\n    self.step(step_log_entry)\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/agents/agents.py\", line 1164, in step\n    raise AgentGenerationError(f\"Error in generating llm output: {e}.\")\ntransformers.agents.agents.AgentGenerationError: Error in generating llm output: (Request ID: Root=1-67d31052-7fe470d11a8dc75b1f9c9759;9e44cc62-a92f-4c24-b273-469588ea205c)\n\nBad request:\nModel requires a Pro subscription; check out hf.co/pricing to learn more. Make sure to include your HF token in your query..\n\u001b[31;20mReached max iterations.\u001b[0m\nNoneType: None\n","output_type":"stream"},{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"'Error in generating final llm output: (Request ID: Root=1-67d31052-2975509007723dab52acd22a;7bf1bad1-488e-4588-90aa-99143166878e)\\n\\nBad request:\\nModel requires a Pro subscription; check out hf.co/pricing to learn more. Make sure to include your HF token in your query..'"},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"# The prompt and output parser were automatically defined, \n# but you can easily inspect them by calling the system_prompt_template \n# on your agent.\n\nprint(agent.system_prompt_template)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-13T17:06:00.649251Z","iopub.execute_input":"2025-03-13T17:06:00.649626Z","iopub.status.idle":"2025-03-13T17:06:00.655927Z","shell.execute_reply.started":"2025-03-13T17:06:00.649599Z","shell.execute_reply":"2025-03-13T17:06:00.654443Z"}},"outputs":[{"name":"stdout","text":"You are an expert assistant who can solve any task using code blobs. You will be given a task to solve as best you can.\nTo do so, you have been given access to a list of tools: these tools are basically Python functions which you can call with code.\nTo solve the task, you must plan forward to proceed in a series of steps, in a cycle of 'Thought:', 'Code:', and 'Observation:' sequences.\n\nAt each step, in the 'Thought:' sequence, you should first explain your reasoning towards solving the task and the tools that you want to use.\nThen in the 'Code:' sequence, you should write the code in simple Python. The code sequence must end with '<end_action>' sequence.\nDuring each intermediate step, you can use 'print()' to save whatever important information you will then need.\nThese print outputs will then appear in the 'Observation:' field, which will be available as input for the next step.\nIn the end you have to return a final answer using the `final_answer` tool.\n\nHere are a few examples using notional tools:\n---\nTask: \"Generate an image of the oldest person in this document.\"\n\nThought: I will proceed step by step and use the following tools: `document_qa` to find the oldest person in the document, then `image_generator` to generate an image according to the answer.\nCode:\n```py\nanswer = document_qa(document=document, question=\"Who is the oldest person mentioned?\")\nprint(answer)\n```<end_action>\nObservation: \"The oldest person in the document is John Doe, a 55 year old lumberjack living in Newfoundland.\"\n\nThought: I will now generate an image showcasing the oldest person.\nCode:\n```py\nimage = image_generator(\"A portrait of John Doe, a 55-year-old man living in Canada.\")\nfinal_answer(image)\n```<end_action>\n\n---\nTask: \"What is the result of the following operation: 5 + 3 + 1294.678?\"\n\nThought: I will use python code to compute the result of the operation and then return the final answer using the `final_answer` tool\nCode:\n```py\nresult = 5 + 3 + 1294.678\nfinal_answer(result)\n```<end_action>\n\n---\nTask: \"Which city has the highest population: Guangzhou or Shanghai?\"\n\nThought: I need to get the populations for both cities and compare them: I will use the tool `search` to get the population of both cities.\nCode:\n```py\npopulation_guangzhou = search(\"Guangzhou population\")\nprint(\"Population Guangzhou:\", population_guangzhou)\npopulation_shanghai = search(\"Shanghai population\")\nprint(\"Population Shanghai:\", population_shanghai)\n```<end_action>\nObservation:\nPopulation Guangzhou: ['Guangzhou has a population of 15 million inhabitants as of 2021.']\nPopulation Shanghai: '26 million (2019)'\n\nThought: Now I know that Shanghai has the highest population.\nCode:\n```py\nfinal_answer(\"Shanghai\")\n```<end_action>\n\n---\nTask: \"What is the current age of the pope, raised to the power 0.36?\"\n\nThought: I will use the tool `wiki` to get the age of the pope, then raise it to the power 0.36.\nCode:\n```py\npope_age = wiki(query=\"current pope age\")\nprint(\"Pope age:\", pope_age)\n```<end_action>\nObservation:\nPope age: \"The pope Francis is currently 85 years old.\"\n\nThought: I know that the pope is 85 years old. Let's compute the result using python code.\nCode:\n```py\npope_current_age = 85 ** 0.36\nfinal_answer(pope_current_age)\n```<end_action>\n\nAbove example were using notional tools that might not exist for you. On top of performing computations in the Python code snippets that you create, you have access to these tools (and no other tool):\n\n<<tool_descriptions>>\n\n<<managed_agents_descriptions>>\n\nHere are the rules you should always follow to solve your task:\n1. Always provide a 'Thought:' sequence, and a 'Code:\n```py' sequence ending with '```<end_action>' sequence, else you will fail.\n2. Use only variables that you have defined!\n3. Always use the right arguments for the tools. DO NOT pass the arguments as a dict as in 'answer = wiki({'query': \"What is the place where James Bond lives?\"})', but use the arguments directly as in 'answer = wiki(query=\"What is the place where James Bond lives?\")'.\n4. Take care to not chain too many sequential tool calls in the same code block, especially when the output format is unpredictable. For instance, a call to search has an unpredictable return format, so do not have another tool call that depends on its output in the same block: rather output results with print() to use them in the next block.\n5. Call a tool only when needed, and never re-do a tool call that you previously did with the exact same parameters.\n6. Don't name any new variable with the same name as a tool: for instance don't name a variable 'final_answer'.\n7. Never create any notional variables in our code, as having these in your logs might derail you from the true variables.\n8. You can use imports in your code, but only from the following list of modules: <<authorized_imports>>\n9. The state persists between code executions: so if in one step you've created variables or imported modules, these will all persist.\n10. Don't give up! You're in charge of solving the task, not providing directions to solve it.\n\nNow Begin! If you solve the task correctly, you will receive a reward of $1,000,000.\n\n","output_type":"stream"}],"execution_count":7},{"cell_type":"markdown","source":"It’s important to explain as clearly as possible the task you want to perform. Every run() operation is independent, and since an agent is powered by an LLM, minor variations in your prompt might yield completely different results. You can also run an agent consecutively for different tasks: each time the attributes agent.task and agent.logs will be re-initialized.\n\n# Code execution\nA Python interpreter executes the code on a set of inputs passed along with your tools. This should be safe because the only functions that can be called are the tools you provided (especially if it’s only tools by Hugging Face) and the print function, so you’re already limited in what can be executed.\n\nThe Python interpreter also doesn’t allow imports by default outside of a safe list, so all the most obvious attacks shouldn’t be an issue. You can still authorize additional imports by passing the authorized modules as a list of strings in argument additional_authorized_imports upon initialization of your ReactCodeAgent or CodeAgent:","metadata":{}},{"cell_type":"code","source":"from transformers import ReactCodeAgent\n\nagent = ReactCodeAgent(tools=[], additional_authorized_imports=['requests', 'bs4'])\nagent.run(\"Could you get me the title of the page at url 'https://huggingface.co/blog'?\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-13T17:07:50.908905Z","iopub.execute_input":"2025-03-13T17:07:50.909431Z","iopub.status.idle":"2025-03-13T17:07:53.690246Z","shell.execute_reply.started":"2025-03-13T17:07:50.909391Z","shell.execute_reply":"2025-03-13T17:07:53.688902Z"}},"outputs":[{"name":"stderr","text":"Failed to load tokenizer for model meta-llama/Meta-Llama-3.1-8B-Instruct: You are trying to access a gated repo.\nMake sure to have access to it at https://huggingface.co/meta-llama/Meta-Llama-3.1-8B-Instruct.\n403 Client Error. (Request ID: Root=1-67d310e7-0c6723736bd8f98c67fd67f3;34a90d34-1d6a-4d07-a909-86f896d211f0)\n\nCannot access gated repo for url https://huggingface.co/meta-llama/Meta-Llama-3.1-8B-Instruct/resolve/main/config.json.\nAccess to model meta-llama/Llama-3.1-8B-Instruct is restricted and you are not in the authorized list. Visit https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct to ask for access.. Loading default tokenizer instead.\n\u001b[32;20;1m======== New task ========\u001b[0m\n\u001b[37;1mCould you get me the title of the page at url 'https://huggingface.co/blog'?\u001b[0m\n\u001b[31;20mError in generating llm output: (Request ID: Root=1-67d310e8-632352053fd86d2e7ada8ba5;f0551b63-1e55-4146-9835-215cfb1f0acb)\n\nBad request:\nModel requires a Pro subscription; check out hf.co/pricing to learn more. Make sure to include your HF token in your query..\u001b[0m\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_http.py\", line 409, in hf_raise_for_status\n    response.raise_for_status()\n  File \"/usr/local/lib/python3.10/dist-packages/requests/models.py\", line 1024, in raise_for_status\n    raise HTTPError(http_error_msg, response=self)\nrequests.exceptions.HTTPError: 400 Client Error: Bad Request for url: https://router.huggingface.co/hf-inference/models/meta-llama/Meta-Llama-3.1-8B-Instruct/v1/chat/completions\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/agents/agents.py\", line 1160, in step\n    llm_output = self.llm_engine(\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/agents/llm_engine.py\", line 134, in __call__\n    response = self.generate(messages, stop_sequences, grammar)\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/agents/llm_engine.py\", line 190, in generate\n    response = self.client.chat_completion(messages, stop=stop_sequences, max_tokens=self.max_tokens)\n  File \"/usr/local/lib/python3.10/dist-packages/huggingface_hub/inference/_client.py\", line 956, in chat_completion\n    data = self._inner_post(request_parameters, stream=stream)\n  File \"/usr/local/lib/python3.10/dist-packages/huggingface_hub/inference/_client.py\", line 321, in _inner_post\n    hf_raise_for_status(response)\n  File \"/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_http.py\", line 464, in hf_raise_for_status\n    raise _format(BadRequestError, message, response) from e\nhuggingface_hub.errors.BadRequestError: (Request ID: Root=1-67d310e8-632352053fd86d2e7ada8ba5;f0551b63-1e55-4146-9835-215cfb1f0acb)\n\nBad request:\nModel requires a Pro subscription; check out hf.co/pricing to learn more. Make sure to include your HF token in your query.\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/agents/agents.py\", line 856, in direct_run\n    self.step(step_log_entry)\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/agents/agents.py\", line 1164, in step\n    raise AgentGenerationError(f\"Error in generating llm output: {e}.\")\ntransformers.agents.agents.AgentGenerationError: Error in generating llm output: (Request ID: Root=1-67d310e8-632352053fd86d2e7ada8ba5;f0551b63-1e55-4146-9835-215cfb1f0acb)\n\nBad request:\nModel requires a Pro subscription; check out hf.co/pricing to learn more. Make sure to include your HF token in your query..\n\u001b[31;20mError in generating llm output: (Request ID: Root=1-67d310e8-36a2eb4413ca22f26aa8eeb9;9fd0f7ac-6f5f-41c5-9cf7-ea720080ee7b)\n\nBad request:\nModel requires a Pro subscription; check out hf.co/pricing to learn more. Make sure to include your HF token in your query..\u001b[0m\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_http.py\", line 409, in hf_raise_for_status\n    response.raise_for_status()\n  File \"/usr/local/lib/python3.10/dist-packages/requests/models.py\", line 1024, in raise_for_status\n    raise HTTPError(http_error_msg, response=self)\nrequests.exceptions.HTTPError: 400 Client Error: Bad Request for url: https://router.huggingface.co/hf-inference/models/meta-llama/Meta-Llama-3.1-8B-Instruct/v1/chat/completions\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/agents/agents.py\", line 1160, in step\n    llm_output = self.llm_engine(\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/agents/llm_engine.py\", line 134, in __call__\n    response = self.generate(messages, stop_sequences, grammar)\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/agents/llm_engine.py\", line 190, in generate\n    response = self.client.chat_completion(messages, stop=stop_sequences, max_tokens=self.max_tokens)\n  File \"/usr/local/lib/python3.10/dist-packages/huggingface_hub/inference/_client.py\", line 956, in chat_completion\n    data = self._inner_post(request_parameters, stream=stream)\n  File \"/usr/local/lib/python3.10/dist-packages/huggingface_hub/inference/_client.py\", line 321, in _inner_post\n    hf_raise_for_status(response)\n  File \"/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_http.py\", line 464, in hf_raise_for_status\n    raise _format(BadRequestError, message, response) from e\nhuggingface_hub.errors.BadRequestError: (Request ID: Root=1-67d310e8-36a2eb4413ca22f26aa8eeb9;9fd0f7ac-6f5f-41c5-9cf7-ea720080ee7b)\n\nBad request:\nModel requires a Pro subscription; check out hf.co/pricing to learn more. Make sure to include your HF token in your query.\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/agents/agents.py\", line 856, in direct_run\n    self.step(step_log_entry)\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/agents/agents.py\", line 1164, in step\n    raise AgentGenerationError(f\"Error in generating llm output: {e}.\")\ntransformers.agents.agents.AgentGenerationError: Error in generating llm output: (Request ID: Root=1-67d310e8-36a2eb4413ca22f26aa8eeb9;9fd0f7ac-6f5f-41c5-9cf7-ea720080ee7b)\n\nBad request:\nModel requires a Pro subscription; check out hf.co/pricing to learn more. Make sure to include your HF token in your query..\n\u001b[31;20mError in generating llm output: (Request ID: Root=1-67d310e8-07ff33f41212d4cb42eb6734;ed7cf973-9009-41d4-bd4d-2b1c5c8b1893)\n\nBad request:\nModel requires a Pro subscription; check out hf.co/pricing to learn more. Make sure to include your HF token in your query..\u001b[0m\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_http.py\", line 409, in hf_raise_for_status\n    response.raise_for_status()\n  File \"/usr/local/lib/python3.10/dist-packages/requests/models.py\", line 1024, in raise_for_status\n    raise HTTPError(http_error_msg, response=self)\nrequests.exceptions.HTTPError: 400 Client Error: Bad Request for url: https://router.huggingface.co/hf-inference/models/meta-llama/Meta-Llama-3.1-8B-Instruct/v1/chat/completions\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/agents/agents.py\", line 1160, in step\n    llm_output = self.llm_engine(\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/agents/llm_engine.py\", line 134, in __call__\n    response = self.generate(messages, stop_sequences, grammar)\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/agents/llm_engine.py\", line 190, in generate\n    response = self.client.chat_completion(messages, stop=stop_sequences, max_tokens=self.max_tokens)\n  File \"/usr/local/lib/python3.10/dist-packages/huggingface_hub/inference/_client.py\", line 956, in chat_completion\n    data = self._inner_post(request_parameters, stream=stream)\n  File \"/usr/local/lib/python3.10/dist-packages/huggingface_hub/inference/_client.py\", line 321, in _inner_post\n    hf_raise_for_status(response)\n  File \"/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_http.py\", line 464, in hf_raise_for_status\n    raise _format(BadRequestError, message, response) from e\nhuggingface_hub.errors.BadRequestError: (Request ID: Root=1-67d310e8-07ff33f41212d4cb42eb6734;ed7cf973-9009-41d4-bd4d-2b1c5c8b1893)\n\nBad request:\nModel requires a Pro subscription; check out hf.co/pricing to learn more. Make sure to include your HF token in your query.\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/agents/agents.py\", line 856, in direct_run\n    self.step(step_log_entry)\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/agents/agents.py\", line 1164, in step\n    raise AgentGenerationError(f\"Error in generating llm output: {e}.\")\ntransformers.agents.agents.AgentGenerationError: Error in generating llm output: (Request ID: Root=1-67d310e8-07ff33f41212d4cb42eb6734;ed7cf973-9009-41d4-bd4d-2b1c5c8b1893)\n\nBad request:\nModel requires a Pro subscription; check out hf.co/pricing to learn more. Make sure to include your HF token in your query..\n\u001b[31;20mError in generating llm output: (Request ID: Root=1-67d310e8-251521600c8d9d374ec46e94;c38b0da7-5ae3-401e-ba90-f99dcf5cd101)\n\nBad request:\nModel requires a Pro subscription; check out hf.co/pricing to learn more. Make sure to include your HF token in your query..\u001b[0m\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_http.py\", line 409, in hf_raise_for_status\n    response.raise_for_status()\n  File \"/usr/local/lib/python3.10/dist-packages/requests/models.py\", line 1024, in raise_for_status\n    raise HTTPError(http_error_msg, response=self)\nrequests.exceptions.HTTPError: 400 Client Error: Bad Request for url: https://router.huggingface.co/hf-inference/models/meta-llama/Meta-Llama-3.1-8B-Instruct/v1/chat/completions\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/agents/agents.py\", line 1160, in step\n    llm_output = self.llm_engine(\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/agents/llm_engine.py\", line 134, in __call__\n    response = self.generate(messages, stop_sequences, grammar)\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/agents/llm_engine.py\", line 190, in generate\n    response = self.client.chat_completion(messages, stop=stop_sequences, max_tokens=self.max_tokens)\n  File \"/usr/local/lib/python3.10/dist-packages/huggingface_hub/inference/_client.py\", line 956, in chat_completion\n    data = self._inner_post(request_parameters, stream=stream)\n  File \"/usr/local/lib/python3.10/dist-packages/huggingface_hub/inference/_client.py\", line 321, in _inner_post\n    hf_raise_for_status(response)\n  File \"/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_http.py\", line 464, in hf_raise_for_status\n    raise _format(BadRequestError, message, response) from e\nhuggingface_hub.errors.BadRequestError: (Request ID: Root=1-67d310e8-251521600c8d9d374ec46e94;c38b0da7-5ae3-401e-ba90-f99dcf5cd101)\n\nBad request:\nModel requires a Pro subscription; check out hf.co/pricing to learn more. Make sure to include your HF token in your query.\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/agents/agents.py\", line 856, in direct_run\n    self.step(step_log_entry)\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/agents/agents.py\", line 1164, in step\n    raise AgentGenerationError(f\"Error in generating llm output: {e}.\")\ntransformers.agents.agents.AgentGenerationError: Error in generating llm output: (Request ID: Root=1-67d310e8-251521600c8d9d374ec46e94;c38b0da7-5ae3-401e-ba90-f99dcf5cd101)\n\nBad request:\nModel requires a Pro subscription; check out hf.co/pricing to learn more. Make sure to include your HF token in your query..\n\u001b[31;20mError in generating llm output: (Request ID: Root=1-67d310e9-27353b6e2d5bb9b4755dcb0b;3ca7f128-9746-477a-bf58-f56078d21814)\n\nBad request:\nModel requires a Pro subscription; check out hf.co/pricing to learn more. Make sure to include your HF token in your query..\u001b[0m\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_http.py\", line 409, in hf_raise_for_status\n    response.raise_for_status()\n  File \"/usr/local/lib/python3.10/dist-packages/requests/models.py\", line 1024, in raise_for_status\n    raise HTTPError(http_error_msg, response=self)\nrequests.exceptions.HTTPError: 400 Client Error: Bad Request for url: https://router.huggingface.co/hf-inference/models/meta-llama/Meta-Llama-3.1-8B-Instruct/v1/chat/completions\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/agents/agents.py\", line 1160, in step\n    llm_output = self.llm_engine(\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/agents/llm_engine.py\", line 134, in __call__\n    response = self.generate(messages, stop_sequences, grammar)\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/agents/llm_engine.py\", line 190, in generate\n    response = self.client.chat_completion(messages, stop=stop_sequences, max_tokens=self.max_tokens)\n  File \"/usr/local/lib/python3.10/dist-packages/huggingface_hub/inference/_client.py\", line 956, in chat_completion\n    data = self._inner_post(request_parameters, stream=stream)\n  File \"/usr/local/lib/python3.10/dist-packages/huggingface_hub/inference/_client.py\", line 321, in _inner_post\n    hf_raise_for_status(response)\n  File \"/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_http.py\", line 464, in hf_raise_for_status\n    raise _format(BadRequestError, message, response) from e\nhuggingface_hub.errors.BadRequestError: (Request ID: Root=1-67d310e9-27353b6e2d5bb9b4755dcb0b;3ca7f128-9746-477a-bf58-f56078d21814)\n\nBad request:\nModel requires a Pro subscription; check out hf.co/pricing to learn more. Make sure to include your HF token in your query.\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/agents/agents.py\", line 856, in direct_run\n    self.step(step_log_entry)\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/agents/agents.py\", line 1164, in step\n    raise AgentGenerationError(f\"Error in generating llm output: {e}.\")\ntransformers.agents.agents.AgentGenerationError: Error in generating llm output: (Request ID: Root=1-67d310e9-27353b6e2d5bb9b4755dcb0b;3ca7f128-9746-477a-bf58-f56078d21814)\n\nBad request:\nModel requires a Pro subscription; check out hf.co/pricing to learn more. Make sure to include your HF token in your query..\n\u001b[31;20mError in generating llm output: (Request ID: Root=1-67d310e9-7c0ae3b92c7b8faa14465f3d;dd251ef4-e182-4afe-a7f8-a441497033eb)\n\nBad request:\nModel requires a Pro subscription; check out hf.co/pricing to learn more. Make sure to include your HF token in your query..\u001b[0m\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_http.py\", line 409, in hf_raise_for_status\n    response.raise_for_status()\n  File \"/usr/local/lib/python3.10/dist-packages/requests/models.py\", line 1024, in raise_for_status\n    raise HTTPError(http_error_msg, response=self)\nrequests.exceptions.HTTPError: 400 Client Error: Bad Request for url: https://router.huggingface.co/hf-inference/models/meta-llama/Meta-Llama-3.1-8B-Instruct/v1/chat/completions\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/agents/agents.py\", line 1160, in step\n    llm_output = self.llm_engine(\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/agents/llm_engine.py\", line 134, in __call__\n    response = self.generate(messages, stop_sequences, grammar)\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/agents/llm_engine.py\", line 190, in generate\n    response = self.client.chat_completion(messages, stop=stop_sequences, max_tokens=self.max_tokens)\n  File \"/usr/local/lib/python3.10/dist-packages/huggingface_hub/inference/_client.py\", line 956, in chat_completion\n    data = self._inner_post(request_parameters, stream=stream)\n  File \"/usr/local/lib/python3.10/dist-packages/huggingface_hub/inference/_client.py\", line 321, in _inner_post\n    hf_raise_for_status(response)\n  File \"/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_http.py\", line 464, in hf_raise_for_status\n    raise _format(BadRequestError, message, response) from e\nhuggingface_hub.errors.BadRequestError: (Request ID: Root=1-67d310e9-7c0ae3b92c7b8faa14465f3d;dd251ef4-e182-4afe-a7f8-a441497033eb)\n\nBad request:\nModel requires a Pro subscription; check out hf.co/pricing to learn more. Make sure to include your HF token in your query.\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/agents/agents.py\", line 856, in direct_run\n    self.step(step_log_entry)\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/agents/agents.py\", line 1164, in step\n    raise AgentGenerationError(f\"Error in generating llm output: {e}.\")\ntransformers.agents.agents.AgentGenerationError: Error in generating llm output: (Request ID: Root=1-67d310e9-7c0ae3b92c7b8faa14465f3d;dd251ef4-e182-4afe-a7f8-a441497033eb)\n\nBad request:\nModel requires a Pro subscription; check out hf.co/pricing to learn more. Make sure to include your HF token in your query..\n\u001b[31;20mReached max iterations.\u001b[0m\nNoneType: None\n","output_type":"stream"},{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"'Error in generating final llm output: (Request ID: Root=1-67d310e9-12cf0ddb605199751c32a4b2;b6b94449-9868-46b6-9afc-5d2ace688125)\\n\\nBad request:\\nModel requires a Pro subscription; check out hf.co/pricing to learn more. Make sure to include your HF token in your query..'"},"metadata":{}}],"execution_count":8},{"cell_type":"markdown","source":"The execution will stop at any code trying to perform an illegal operation or if there is a regular Python error with the code generated by the agent.","metadata":{}},{"cell_type":"markdown","source":"# Tools\nA tool is an atomic function to be used by an agent.\n\nYou can for instance check the PythonInterpreterTool: it has a name, a description, input descriptions, an output type, and a __call__ method to perform the action.\n\nWhen the agent is initialized, the tool attributes are used to generate a tool description which is baked into the agent’s system prompt. This lets the agent know which tools it can use and why.\n\n# Default toolbox\nTransformers comes with a default toolbox for empowering agents, that you can add to your agent upon initialization with argument add_base_tools = True:\n\nDocument question answering: given a document (such as a PDF) in image format, answer a question on this document (Donut)\nImage question answering: given an image, answer a question on this image (VILT)\n\nSpeech to text: given an audio recording of a person talking, transcribe the speech into text (Whisper)\n\nText to speech: convert text to speech (SpeechT5)\n\nTranslation: translates a given sentence from source language to target language.\n\nDuckDuckGo search*: performs a web search using DuckDuckGo browser.\n\nPython code interpreter: runs your the LLM generated Python code in a secure environment. This tool will only be added to ReactJsonAgent if you initialize it with add_base_tools=True, since code-based agent can already natively execute Python code\n\nYou can manually use a tool by calling the load_tool() function and a task to perform.","metadata":{}},{"cell_type":"code","source":"from transformers import load_tool\n\ntool = load_tool(\"text-to-speech\")\naudio = tool(\"This is a text to speech tool\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Create a new tool\nYou can create your own tool for use cases not covered by the default tools from Hugging Face. For example, let’s create a tool that returns the most downloaded model for a given task from the Hub.\n\nYou’ll start with the code below.","metadata":{}},{"cell_type":"code","source":"from huggingface_hub import list_models\n\ntask = \"text-classification\"\n\nmodel = next(iter(list_models(filter=task, sort=\"downloads\", direction=-1)))\nprint(model.id)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-13T17:10:57.249184Z","iopub.execute_input":"2025-03-13T17:10:57.249553Z","iopub.status.idle":"2025-03-13T17:10:58.040274Z","shell.execute_reply.started":"2025-03-13T17:10:57.249527Z","shell.execute_reply":"2025-03-13T17:10:58.039228Z"}},"outputs":[{"name":"stdout","text":"cross-encoder/ms-marco-MiniLM-L6-v2\n","output_type":"stream"}],"execution_count":10},{"cell_type":"markdown","source":"This code can quickly be converted into a tool, just by wrapping it in a function and adding the tool decorator:","metadata":{}},{"cell_type":"code","source":"from transformers import tool\n\n@tool\ndef model_download_tool(task: str) -> str:\n    \"\"\"\n    This is a tool that returns the most downloaded model of a given task on the Hugging Face Hub.\n    It returns the name of the checkpoint.\n\n    Args:\n        task: The task for which\n    \"\"\"\n    model = next(iter(list_models(filter=\"text-classification\", sort=\"downloads\", direction=-1)))\n    return model.id","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-13T17:12:02.414991Z","iopub.execute_input":"2025-03-13T17:12:02.415467Z","iopub.status.idle":"2025-03-13T17:12:02.422228Z","shell.execute_reply.started":"2025-03-13T17:12:02.415435Z","shell.execute_reply":"2025-03-13T17:12:02.420866Z"}},"outputs":[],"execution_count":11},{"cell_type":"markdown","source":"The function needs:\n\n1) A clear name. The name usually describes what the tool does. Since the code returns the model with the most downloads for a task, let’s put model_download_tool.\n2) Type hints on both inputs and output\n3) A description, that includes an ‘Args:’ part where each argument is described (without a type indication this time, it will be pulled from the type hint). All these will be automatically baked into the agent’s system prompt upon initialization: so strive to make them as clear as possible!","metadata":{}},{"cell_type":"code","source":"from transformers import CodeAgent\nagent = CodeAgent(tools=[model_download_tool], llm_engine=llm_engine)\nagent.run(\n    \"Can you give me the name of the model that has the most downloads in the 'text-to-video' task on the Hugging Face Hub?\"\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}