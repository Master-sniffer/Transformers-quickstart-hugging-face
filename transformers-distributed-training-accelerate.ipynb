{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# TAKEN FROM https://huggingface.co/docs/transformers/accelerate","metadata":{}},{"cell_type":"code","source":"!pip install --upgrade --quiet transformers datasets evaluate soundfile librosa  \n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-03-13T15:04:05.938674Z","iopub.execute_input":"2025-03-13T15:04:05.938973Z","iopub.status.idle":"2025-03-13T15:04:41.728846Z","shell.execute_reply.started":"2025-03-13T15:04:05.938944Z","shell.execute_reply":"2025-03-13T15:04:41.727357Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m44.0/44.0 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m10.0/10.0 MB\u001b[0m \u001b[31m59.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m485.4/485.4 kB\u001b[0m \u001b[31m18.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m35.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m260.7/260.7 kB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"Distributed training with ğŸ¤— Accelerate\nAs models get bigger, parallelism has emerged as a strategy for training larger models on limited hardware and accelerating training speed by several orders of magnitude. At Hugging Face, we created the ğŸ¤— Accelerate library to help users easily train a ğŸ¤— Transformers model on any type of distributed setup, whether it is multiple GPUâ€™s on one machine or multiple GPUâ€™s across several machines. In this tutorial, learn how to customize your native PyTorch training loop to enable training in a distributed environment.","metadata":{}},{"cell_type":"code","source":"!pip install --upgrade --quiet accelerate","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-13T15:05:24.145833Z","iopub.execute_input":"2025-03-13T15:05:24.146229Z","iopub.status.idle":"2025-03-13T15:05:29.546002Z","shell.execute_reply.started":"2025-03-13T15:05:24.146193Z","shell.execute_reply":"2025-03-13T15:05:29.544585Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m345.1/345.1 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25h","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"from accelerate import Accelerator\n\naccelerator = Accelerator()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-13T15:05:39.827706Z","iopub.execute_input":"2025-03-13T15:05:39.828126Z","iopub.status.idle":"2025-03-13T15:05:45.110002Z","shell.execute_reply.started":"2025-03-13T15:05:39.828093Z","shell.execute_reply":"2025-03-13T15:05:45.108770Z"}},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":"# Prepare to accelerate\nThe next step is to pass all the relevant training objects to the prepare method. This includes your training and evaluation DataLoaders, a model and an optimizer:","metadata":{}},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import DataLoader, random_split\nfrom torchvision.datasets import MNIST\nfrom torchvision.transforms import ToTensor\nimport torch.nn as nn\nimport torch.optim as optim\nfrom accelerate import Accelerator\nfrom torch.optim.lr_scheduler import StepLR\nfrom tqdm.auto import tqdm\n\n# Custom collate function to convert each batch from a tuple to a dictionary\ndef custom_collate(batch):\n    images, labels = zip(*batch)\n    images = torch.stack(images)\n    labels = torch.tensor(labels)\n    return {\"x\": images, \"labels\": labels}\n\n# Initialize Accelerator\naccelerator = Accelerator()\n\n# Download and load the MNIST dataset\ndataset = MNIST(root=\"./data\", download=True, transform=ToTensor())\n\n# Split the dataset into training (80%) and evaluation (20%) sets\ntrain_size = int(0.8 * len(dataset))\neval_size = len(dataset) - train_size\ntrain_dataset, eval_dataset = random_split(dataset, [train_size, eval_size])\n\n# Create dataloaders with the custom collate function\ntrain_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=True, collate_fn=custom_collate)\neval_dataloader = DataLoader(eval_dataset, batch_size=64, shuffle=False, collate_fn=custom_collate)\n\n# Define a simple class to wrap the output so that you can use .loss in your training loop\nclass ModelOutput:\n    def __init__(self, loss, logits):\n        self.loss = loss\n        self.logits = logits\n\n# Define a simple neural network model for MNIST classification\nclass SimpleNet(nn.Module):\n    def __init__(self):\n        super(SimpleNet, self).__init__()\n        self.flatten = nn.Flatten()\n        self.linear = nn.Linear(28 * 28, 10)  # MNIST images are 28x28 and there are 10 classes\n        self.loss_fn = nn.CrossEntropyLoss()\n    \n    def forward(self, x, labels=None):\n        x = self.flatten(x)\n        logits = self.linear(x)\n        loss = None\n        if labels is not None:\n            loss = self.loss_fn(logits, labels)\n        return ModelOutput(loss, logits)\n\nmodel = SimpleNet()\n\n# Define optimizer and learning rate scheduler\noptimizer = optim.Adam(model.parameters(), lr=0.001)\nlr_scheduler = StepLR(optimizer, step_size=1, gamma=0.95)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-13T15:17:43.064188Z","iopub.execute_input":"2025-03-13T15:17:43.064604Z","iopub.status.idle":"2025-03-13T15:17:43.190490Z","shell.execute_reply.started":"2025-03-13T15:17:43.064569Z","shell.execute_reply":"2025-03-13T15:17:43.189431Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"train_dataloader, eval_dataloader, model, optimizer = accelerator.prepare(\n    train_dataloader, eval_dataloader, model, optimizer\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-13T15:17:45.842762Z","iopub.execute_input":"2025-03-13T15:17:45.843090Z","iopub.status.idle":"2025-03-13T15:17:45.851301Z","shell.execute_reply.started":"2025-03-13T15:17:45.843063Z","shell.execute_reply":"2025-03-13T15:17:45.849883Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"progress_bar = tqdm(total=len(train_dataloader) * 10)  # assuming 100 epochs\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-13T15:19:14.267153Z","iopub.execute_input":"2025-03-13T15:19:14.267563Z","iopub.status.idle":"2025-03-13T15:19:14.285705Z","shell.execute_reply.started":"2025-03-13T15:19:14.267529Z","shell.execute_reply":"2025-03-13T15:19:14.284497Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/7500 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"677f93caf3c1488fbe42450a43d8fc55"}},"metadata":{}}],"execution_count":18},{"cell_type":"code","source":"\n\n# Training loop that uses model(**batch)\nnum_epochs = 10\nfor epoch in range(num_epochs):\n    model.train()\n    for batch in train_dataloader:\n        # Since our custom collate returns a dict with keys \"x\" and \"labels\",\n        # this works with model(**batch)\n        outputs = model(**batch)\n        loss = outputs.loss\n        \n        accelerator.backward(loss)\n        optimizer.step()\n        lr_scheduler.step()\n        optimizer.zero_grad()\n        progress_bar.update(1)\n        \n    print(f\"Epoch {epoch+1}/{num_epochs} completed\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-13T15:19:14.668058Z","iopub.execute_input":"2025-03-13T15:19:14.668505Z","iopub.status.idle":"2025-03-13T15:20:33.607025Z","shell.execute_reply.started":"2025-03-13T15:19:14.668460Z","shell.execute_reply":"2025-03-13T15:20:33.605938Z"}},"outputs":[{"name":"stdout","text":"Epoch 1/10 completed\nEpoch 2/10 completed\nEpoch 3/10 completed\nEpoch 4/10 completed\nEpoch 5/10 completed\nEpoch 6/10 completed\nEpoch 7/10 completed\nEpoch 8/10 completed\nEpoch 9/10 completed\nEpoch 10/10 completed\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"def custom_collate(batch):\n    processed_input_ids = []\n    for item in batch:\n        tensor = item[0]\n        # If tensor has an extra dimension (e.g. shape: [1, seq_length]), remove it.\n        if tensor.ndim > 1 and tensor.shape[0] == 1:\n            tensor = tensor.squeeze(0)\n        processed_input_ids.append(tensor)\n    input_ids = torch.stack(processed_input_ids).long()\n    # Now input_ids should be of shape (batch_size, sequence_length)\n    attention_mask = torch.ones(input_ids.size(0), input_ids.size(1), dtype=torch.long)\n    labels = torch.tensor([item[1] for item in batch], dtype=torch.long)\n    return {\"input_ids\": input_ids, \"attention_mask\": attention_mask, \"labels\": labels}\n\n\ntrain_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=True, collate_fn=custom_collate)\neval_dataloader = DataLoader(eval_dataset, batch_size=64, shuffle=False, collate_fn=custom_collate)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-13T15:29:50.671485Z","iopub.execute_input":"2025-03-13T15:29:50.671868Z","iopub.status.idle":"2025-03-13T15:29:50.679481Z","shell.execute_reply.started":"2025-03-13T15:29:50.671837Z","shell.execute_reply":"2025-03-13T15:29:50.677963Z"}},"outputs":[],"execution_count":29},{"cell_type":"code","source":"from accelerate import Accelerator\nfrom transformers import AdamW, AutoModelForSequenceClassification, get_scheduler\n\naccelerator = Accelerator()\n\ncheckpoint = \"distilbert-base-uncased\"\nnum_labels = 2  # Binary classification\nmodel = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=num_labels)\n\n\noptimizer = AdamW(model.parameters(), lr=3e-5)\n\ndevice = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\nmodel.to(device)\n\ntrain_dataloader, eval_dataloader, model, optimizer = accelerator.prepare(\n     train_dataloader, eval_dataloader, model, optimizer)\n\nnum_epochs = 3\nnum_training_steps = num_epochs * len(train_dataloader)\nlr_scheduler = get_scheduler(\n  \"linear\",\n  optimizer=optimizer,\n  num_warmup_steps=0,\n  num_training_steps=num_training_steps\n)\n\nprogress_bar = tqdm(range(num_training_steps))\n\nmodel.train()\nfor epoch in range(num_epochs):\n  for batch in train_dataloader:\n      #batch = {k: v.to(device) for k, v in batch.items()}\n      outputs = model(**batch)\n      loss = outputs.loss\n      #loss.backward()\n      accelerator.backward(loss)\n\n      optimizer.step()\n      lr_scheduler.step()\n      optimizer.zero_grad()\n      progress_bar.update(1)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}