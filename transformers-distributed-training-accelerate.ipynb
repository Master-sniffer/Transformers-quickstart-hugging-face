{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# TAKEN FROM https://huggingface.co/docs/transformers/accelerate","metadata":{}},{"cell_type":"code","source":"!pip install --upgrade --quiet transformers datasets evaluate soundfile librosa  \n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-03-13T15:04:05.938674Z","iopub.execute_input":"2025-03-13T15:04:05.938973Z","iopub.status.idle":"2025-03-13T15:04:41.728846Z","shell.execute_reply.started":"2025-03-13T15:04:05.938944Z","shell.execute_reply":"2025-03-13T15:04:41.727357Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.0/44.0 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.0/10.0 MB\u001b[0m \u001b[31m59.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m485.4/485.4 kB\u001b[0m \u001b[31m18.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m35.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m260.7/260.7 kB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"Distributed training with 🤗 Accelerate\nAs models get bigger, parallelism has emerged as a strategy for training larger models on limited hardware and accelerating training speed by several orders of magnitude. At Hugging Face, we created the 🤗 Accelerate library to help users easily train a 🤗 Transformers model on any type of distributed setup, whether it is multiple GPU’s on one machine or multiple GPU’s across several machines. In this tutorial, learn how to customize your native PyTorch training loop to enable training in a distributed environment.","metadata":{}},{"cell_type":"code","source":"!pip install --upgrade --quiet accelerate","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-13T15:05:24.145833Z","iopub.execute_input":"2025-03-13T15:05:24.146229Z","iopub.status.idle":"2025-03-13T15:05:29.546002Z","shell.execute_reply.started":"2025-03-13T15:05:24.146193Z","shell.execute_reply":"2025-03-13T15:05:29.544585Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m345.1/345.1 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25h","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"from accelerate import Accelerator\n\naccelerator = Accelerator()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-13T15:05:39.827706Z","iopub.execute_input":"2025-03-13T15:05:39.828126Z","iopub.status.idle":"2025-03-13T15:05:45.110002Z","shell.execute_reply.started":"2025-03-13T15:05:39.828093Z","shell.execute_reply":"2025-03-13T15:05:45.108770Z"}},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":"# Prepare to accelerate\nThe next step is to pass all the relevant training objects to the prepare method. This includes your training and evaluation DataLoaders, a model and an optimizer:","metadata":{}},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import DataLoader, random_split\nfrom torchvision.datasets import MNIST\nfrom torchvision.transforms import ToTensor\nimport torch.nn as nn\nimport torch.optim as optim\nfrom accelerate import Accelerator\nfrom torch.optim.lr_scheduler import StepLR\nfrom tqdm.auto import tqdm\n\n# Custom collate function to convert each batch from a tuple to a dictionary\ndef custom_collate(batch):\n    images, labels = zip(*batch)\n    images = torch.stack(images)\n    labels = torch.tensor(labels)\n    return {\"x\": images, \"labels\": labels}\n\n# Initialize Accelerator\naccelerator = Accelerator()\n\n# Download and load the MNIST dataset\ndataset = MNIST(root=\"./data\", download=True, transform=ToTensor())\n\n# Split the dataset into training (80%) and evaluation (20%) sets\ntrain_size = int(0.8 * len(dataset))\neval_size = len(dataset) - train_size\ntrain_dataset, eval_dataset = random_split(dataset, [train_size, eval_size])\n\n# Create dataloaders with the custom collate function\ntrain_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=True, collate_fn=custom_collate)\neval_dataloader = DataLoader(eval_dataset, batch_size=64, shuffle=False, collate_fn=custom_collate)\n\n# Define a simple class to wrap the output so that you can use .loss in your training loop\nclass ModelOutput:\n    def __init__(self, loss, logits):\n        self.loss = loss\n        self.logits = logits\n\n# Define a simple neural network model for MNIST classification\nclass SimpleNet(nn.Module):\n    def __init__(self):\n        super(SimpleNet, self).__init__()\n        self.flatten = nn.Flatten()\n        self.linear = nn.Linear(28 * 28, 10)  # MNIST images are 28x28 and there are 10 classes\n        self.loss_fn = nn.CrossEntropyLoss()\n    \n    def forward(self, x, labels=None):\n        x = self.flatten(x)\n        logits = self.linear(x)\n        loss = None\n        if labels is not None:\n            loss = self.loss_fn(logits, labels)\n        return ModelOutput(loss, logits)\n\nmodel = SimpleNet()\n\n# Define optimizer and learning rate scheduler\noptimizer = optim.Adam(model.parameters(), lr=0.001)\nlr_scheduler = StepLR(optimizer, step_size=1, gamma=0.95)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-13T15:17:43.064188Z","iopub.execute_input":"2025-03-13T15:17:43.064604Z","iopub.status.idle":"2025-03-13T15:17:43.190490Z","shell.execute_reply.started":"2025-03-13T15:17:43.064569Z","shell.execute_reply":"2025-03-13T15:17:43.189431Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"train_dataloader, eval_dataloader, model, optimizer = accelerator.prepare(\n    train_dataloader, eval_dataloader, model, optimizer\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-13T15:17:45.842762Z","iopub.execute_input":"2025-03-13T15:17:45.843090Z","iopub.status.idle":"2025-03-13T15:17:45.851301Z","shell.execute_reply.started":"2025-03-13T15:17:45.843063Z","shell.execute_reply":"2025-03-13T15:17:45.849883Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"progress_bar = tqdm(total=len(train_dataloader) * 10)  # assuming 100 epochs\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-13T15:19:14.267153Z","iopub.execute_input":"2025-03-13T15:19:14.267563Z","iopub.status.idle":"2025-03-13T15:19:14.285705Z","shell.execute_reply.started":"2025-03-13T15:19:14.267529Z","shell.execute_reply":"2025-03-13T15:19:14.284497Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/7500 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"677f93caf3c1488fbe42450a43d8fc55"}},"metadata":{}}],"execution_count":18},{"cell_type":"code","source":"\n\n# Training loop that uses model(**batch)\nnum_epochs = 10\nfor epoch in range(num_epochs):\n    model.train()\n    for batch in train_dataloader:\n        # Since our custom collate returns a dict with keys \"x\" and \"labels\",\n        # this works with model(**batch)\n        outputs = model(**batch)\n        loss = outputs.loss\n        \n        accelerator.backward(loss)\n        optimizer.step()\n        lr_scheduler.step()\n        optimizer.zero_grad()\n        progress_bar.update(1)\n        \n    print(f\"Epoch {epoch+1}/{num_epochs} completed\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-13T15:19:14.668058Z","iopub.execute_input":"2025-03-13T15:19:14.668505Z","iopub.status.idle":"2025-03-13T15:20:33.607025Z","shell.execute_reply.started":"2025-03-13T15:19:14.668460Z","shell.execute_reply":"2025-03-13T15:20:33.605938Z"}},"outputs":[{"name":"stdout","text":"Epoch 1/10 completed\nEpoch 2/10 completed\nEpoch 3/10 completed\nEpoch 4/10 completed\nEpoch 5/10 completed\nEpoch 6/10 completed\nEpoch 7/10 completed\nEpoch 8/10 completed\nEpoch 9/10 completed\nEpoch 10/10 completed\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"def custom_collate(batch):\n    processed_input_ids = []\n    for item in batch:\n        tensor = item[0]\n        # If tensor has an extra dimension (e.g. shape: [1, seq_length]), remove it.\n        if tensor.ndim > 1 and tensor.shape[0] == 1:\n            tensor = tensor.squeeze(0)\n        processed_input_ids.append(tensor)\n    input_ids = torch.stack(processed_input_ids).long()\n    # Now input_ids should be of shape (batch_size, sequence_length)\n    attention_mask = torch.ones(input_ids.size(0), input_ids.size(1), dtype=torch.long)\n    labels = torch.tensor([item[1] for item in batch], dtype=torch.long)\n    return {\"input_ids\": input_ids, \"attention_mask\": attention_mask, \"labels\": labels}\n\n\ntrain_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=True, collate_fn=custom_collate)\neval_dataloader = DataLoader(eval_dataset, batch_size=64, shuffle=False, collate_fn=custom_collate)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-13T15:29:50.671485Z","iopub.execute_input":"2025-03-13T15:29:50.671868Z","iopub.status.idle":"2025-03-13T15:29:50.679481Z","shell.execute_reply.started":"2025-03-13T15:29:50.671837Z","shell.execute_reply":"2025-03-13T15:29:50.677963Z"}},"outputs":[],"execution_count":29},{"cell_type":"code","source":"from accelerate import Accelerator\nfrom transformers import AdamW, AutoModelForSequenceClassification, get_scheduler\n\naccelerator = Accelerator()\n\ncheckpoint = \"distilbert-base-uncased\"\nnum_labels = 2  # Binary classification\nmodel = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=num_labels)\n\n\noptimizer = AdamW(model.parameters(), lr=3e-5)\n\ndevice = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\nmodel.to(device)\n\ntrain_dataloader, eval_dataloader, model, optimizer = accelerator.prepare(\n     train_dataloader, eval_dataloader, model, optimizer)\n\nnum_epochs = 3\nnum_training_steps = num_epochs * len(train_dataloader)\nlr_scheduler = get_scheduler(\n  \"linear\",\n  optimizer=optimizer,\n  num_warmup_steps=0,\n  num_training_steps=num_training_steps\n)\n\nprogress_bar = tqdm(range(num_training_steps))\n\nmodel.train()\nfor epoch in range(num_epochs):\n  for batch in train_dataloader:\n      #batch = {k: v.to(device) for k, v in batch.items()}\n      outputs = model(**batch)\n      loss = outputs.loss\n      #loss.backward()\n      accelerator.backward(loss)\n\n      optimizer.step()\n      lr_scheduler.step()\n      optimizer.zero_grad()\n      progress_bar.update(1)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}